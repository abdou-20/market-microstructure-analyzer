# Model Architecture Configuration

# Model type
model_type: 'transformer'  # 'transformer', 'lstm', 'hybrid'

# Transformer configuration
d_model: 256
num_heads: 8
num_layers: 6
dropout: 0.1
max_seq_length: 512

# LSTM configuration
hidden_size: 256
num_lstm_layers: 2
bidirectional: true

# Common model settings
output_size: 1
activation: 'relu'
batch_norm: true
layer_norm: true

# Attention mechanism
use_attention: true
attention_type: 'multihead'  # 'multihead', 'self', 'cross'